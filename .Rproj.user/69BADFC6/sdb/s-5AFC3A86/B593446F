{
    "contents" : "---\ntitle: \"Problem Set 1 Answers\"\nauthor: \"ylelkes\"\ndate: \"September 14, 2015\"\noutput: html_document\n---\nFor now, copy and paste your commands and results (including figures) into a word document.\n\n1. Suppose you track your commute times for two weeks (10 days) and you find the following times in minutes: 17 16 20 24 22 15 21 15 17 22\n\n```{r}\ncommute <- c(17, 16,20,24,22,15,21,15,17,22)\n```  \n  \n  \n\t* Use the function max to find the longest commute time, the function mean to find the average and the function min to find the minimum.\n\n```{r}\nmax(commute)\nmean(commute)\nmin(commute)\n```  \n\n* Oops, the 24 was a mistake. It should have been 18. How can you fix this without rewriting the vector? Do so, and then find the new average.\n```{r}\ncommute[4]=18\ncommute[commute==24]=18\ncommute <- car::recode(commute,\"24=18\")\nmean(commute)\n```  \n\n\n* How many times was your commute 20 minutes or more? Hint: try using sum() and a logical statement. [Read here about logical operators](http://www.statmethods.net/management/operators.html)\n\n```{r}\ncommute>20\nsum(commute>20)\n```\n\n* What percent of your commutes are less than 17 minutes? \n```{r}\ncommute>20\nsum(commute<17)/length(commute)*100\nscales::percent(sum(commute<17)/length(commute))\n```\n\n* Replace the 5th element of the vector with NA. Calculate the new mean. \n\n```{r}\ncommute[5]<- NA\nmean(commute)\nmean(commute,na.rm=T)\n```\n\n2. The Crimean War\n\t\n\t* Install the HistData package and load Nightingale. Read about it [here](https://vincentarelbundock.github.io/Rdatasets/doc/HistData/Nightingale.html)\n\t* How many people died of Wounds? How many people died of Disease?\n```{r}\nlibrary(HistData)\ndata(Nightingale)\nsum(Nightingale$Wounds)\nsum(Nightingale$Disease)\n```\n\t* Create a piechart using the pie() command. You should have a slice for # of ppl dying of wounds, # of ppl dying of disease, # of ppl dying of other. Make sure to label all the slices. Give your graph a title. If you wish, change the colors. \n\n```{r}\npie(x = c(sum(Nightingale$Wounds),sum(Nightingale$Disease),sum(Nightingale$Other)),labels = c(\"Wounds\",\"Disease\",\"Other\"),main = \"Causes of Death in the Crimean War\",col = c(\"red\",\"blue\",\"green\"))\n\nwith(Nightingale, pie(x = c(sum(Wounds),sum(Disease),sum(Other)),labels = c(\"Wounds\",\"Disease\",\"Other\"),main = \"Causes of Death in the Crimean War\",col = c(\"red\",\"blue\",\"green\")))\n```\n\n\n3. Loading data into R\n\t* We didn't get to this in class, but loading data into R will be central to your life as a data analysist. \n\t* Download the following files into your working directory (hint: use getwd())\n\t\t* If you want to make it more challenging, read it directly from the web (without downloading it to your working directory).  \n\t\t* https://raw.githubusercontent.com/ylelkes/R_wav/master/data%20examples/Countries-Europe.csv\n\t\t* https://github.com/ylelkes/R_wav/blob/master/data%20examples/child_data.sav?raw=true\n\t\t* https://github.com/ylelkes/R_wav/blob/master/data%20examples/GaltonFamilies.dta?raw=true\n    \n```{r}\nlibrary(RCurl)\neurope <- rio::import((\"https://rawgit.com/ylelkes/R_wav/master/data%20examples/Countries-Europe.csv\"))\nchilddata <- rio::import(\"https://rawgit.com/ylelkes/R_wav/master/data%20examples/child_data.sav\")\ngalton <- rio::import(\"https://rawgit.com/ylelkes/R_wav/master/data%20examples/GaltonFamilies.dta\")\n```    \n\t* Using read.csv, the foreign package, or haven load each one of these datasets into R.\n\t* For: \"Countries-Europe.csv\" (let's call that object europe)\n\t\t* What is the median population of Europe?\n```{r}\nmedian(europe$population)\n```\n* What is the mean population/land area\n\n```{r}\nmean(europe$population/europe[,7])\nmean(europe$population/europe$pop)\nnames(europe)\n```\n\n\n* If you replace X and Y in the following, you will get most populated country in Europe:\n\t\t\t``` \n\t\t\teurope$X[europe$Y==max(europe$population)]\n\t\t\t```\n```{r}\neurope$name[europe$population==max(europe$population)]\n```\n\n\n* What variable is X? What variable is Y?\n\t\t* In your own words, describe what this command is doing..\n\t\n  \n  * For GaltonFamilies.dta:\n\t\t* using the cor() command, what is the correlation between a child's height and his mother's height and what is the correlation between the child's height and the father's height? \n\n```{r}\nfatherchild <- cor(galton$childHeight,galton$father)\nmotherchild <- cor(galton$childHeight,galton$mother)\n\ncortab <- cor(childdata) <- cor(data.frame(galton$childHeight,galton$mother,galton$father))\n\nout <- summary(lm(childHeight~mother+father,galton))\nout$coefficients[2,c(1,4)]\n```\n\t\t* Use a logical statement, get R to confirm that the first correlation does not equal the second correlation.\n```{r}\n\nfatherchild == motherchild\n```\n\n* Using lm(), is there a relationship between the father's height and the number of children he has?\n\t\n```{r}\nsummary(lm(father~childHeight,galton))\n```\n\n* For child_data.sav\n\t\t* What is the memory span of child with the highest IQ?\n\n```{r}\n\nchilddata$MEM_SPAN[childdata$IQ==max(childdata$IQ)]\n```\n\n\n* Create a correlation table for the entire dataset\n\n```{r}\ncortab <- cor(childdata)\n```\n\n\t\t* From the correlation table, extract the correlation between memory span & IQ, the correlation between age and reading ability, and the correlation between IQ and reading ability into a vector\n\n```{r}\nc(cortab[2,3],cortab[1,4],cortab[3,4])\n```\n\n* write that vector to a csv file.\n```{r}\nwrite.csv(c(cortab[2,3],cortab[1,4],cortab[3,4]),file=\"cortabl.csv\")\n```\n\n\t\n\n\n\n\n\n",
    "created" : 1442216626890.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2494117793",
    "id" : "B593446F",
    "lastKnownWriteTime" : 1442931785,
    "path" : "~/Dropbox/R_wav/Problem Sets/problemset1answers.Rmd",
    "project_path" : "Problem Sets/problemset1answers.Rmd",
    "properties" : {
        "tempName" : "Untitled6"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}